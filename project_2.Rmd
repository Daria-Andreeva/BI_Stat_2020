---
title: "Проект 2"
output: html_document
---
# Подготовка к работе
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Нам понадобятся следующие пакеты. 

* [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html)
* [ggplot2](https://cran.r-project.org/web/packages/ggplot2/index.html)
* [corrplot](https://cran.r-project.org/web/packages/corrplot/index.html)
* [lattice](https://cran.r-project.org/web/packages/lattice/index.html)
* [caTools](https://cran.r-project.org/web/packages/caTools/index.html)
* [plotly](https://cran.r-project.org/web/packages/plotly/index.html)
* [MASS](https://cran.r-project.org/web/packages/MASS/index.html)
* [tidyr](https://www.rdocumentation.org/packages/tidyr/versions/0.8.3)


```{r packages, include=FALSE}
library(car)
library(corrplot) #for visualisation of correlation
library(lattice) #for visualisation
library(ggplot2) #for visualisation
library(caTools) #for splittind data into testing and training data
library(dplyr) #manipulating dataframe
library(plotly) #converting ggplot to plotly
library(MASS)
library(tidyr) #for visualisation

theme_set(theme_bw())
```

## 2. Подготовим данные
Проверим отсутсвующие значения и выбросим их при необходимости.
```{r}
numberOfNA <- length(which(is.na(Boston)==T))
if(numberOfNA>0) {
  Boston <- Boston[complete.cases(Boston),]
}
```

Создадим данные для тестирования модели. 
```{r}
set.seed(123)
split <- sample.split(Boston,SplitRatio = 0.75) #assigns booleans to a new coloumn based on the split ratio
train <- subset(Boston,split==TRUE)
test <- subset(Boston,split==FALSE)
```

## 3. Разведочный анализ.
Посмотрим структуру датасета и базовую статистику.
```{r}
str(Boston)

summary(Boston)
```

Здесь мы можем видеть, что переменные «crim» и «black» принимают широкий диапазон значений.

Переменные «crim», «zn», «rm» и «black» имеют большую разницу между их медианной и средним значением, что указывает на множество выбросов в соответствующих переменных.

```{r}
par(mfrow = c(1, 4))
boxplot(Boston$crim, main='crim',col='Sky Blue')
boxplot(Boston$zn, main='zn',col='Sky Blue')
boxplot(Boston$rm, main='rm',col='Sky Blue')
boxplot(Boston$black, main='black',col='Sky Blue')
```

Как и было предсказано, мы видим много выбросов.

## Посчитаем корреляцию между признаками.

```{r}
corr_matrix <- cor(Boston)
corrplot.mixed(corr_matrix)
```

Из корреляционной матрицы можно сделать следующие наблюдения:

1. Средняя стоимость домов, занимаемых владельцами, увеличивается по мере увеличения среднего количества комнат на жилище и уменьшается, если процент населения с более низким статусом в этом районе увеличивается.
2. Концентрация NOx или оксидов азота (ppm) возрастает с увеличением доли акров, не относящихся к розничной торговле, на город и доли жилых единиц, построенных до 1940 года.
3. rad и tax имеют сильную положительную корреляцию 0,91, что означает, что по мере увеличения доступности радиальных автомагистралей, полная ставка налога на имущество из расчета на $ 10 000 также увеличивается.
4. crim тесно связан с переменными rad и tax, что означает, что по мере увеличения доступности радиальных автомагистралей увеличивается уровень преступности на душу населения.
5. indus имеет сильную положительную корреляцию с NOx, что подтверждает мнение о высокой концентрации оксидов азота в промышленных зонах.

## Визуализируем данные.
```{r warning=FALSE}
Boston %>%
  gather(key, val, -medv) %>%
  ggplot(aes(x = val, y = medv)) +
  geom_point() +
  stat_smooth(method = "lm", se = TRUE, col = "blue") +
  facet_wrap(~key, scales = "free") +
  theme_gray() +
  ggtitle("Scatter plot of dependent variables vs Median Value (medv)") 
```


### 4. Построение модели, прогнозирующей стоимость дома
Посмотрим распределение переменной medv.
```{r}
qplot(x=medv,data=Boston,geom='histogram')

```

На данной гистограмме распределения средней цены дома мы видим, что данные немного смещены влево. Однако, линейную регрессию нельзя строить на ненормально распределенных данных. Чтобы исправить распределние прологарифмируем параметр medv.

```{r}
qplot(x=log(medv),data=Boston,geom='histogram')
```

После логарифмирования у нас остался небольшой сдвиг вправо, но распределение выглядит более "нормальным".

### Задание 1. Построение полной модели.
Стандартизируем предикторы:
```{r}
# сделаем наличие реки фактором 
Boston$chas <- as.factor(Boston$chas)
stand_Boston <- Boston %>% mutate_at(c('crim', 'zn', 'indus', 'nox', 'age', 'dis', 'tax', 'ptratio', 'black', 'lstat'), scale)
```

```{r}
model1_stand = lm(log(medv)~., data= stand_Boston)
summary(model1_stand)

par(mfrow=c(2,2))
plot(model1_stand)
```

Полная модель:

+ R-squared составляет 0.7841

+ F-statistic составляет 142.1

Полная модель объясняет 78% изменчивости. Протестируем, есть ли параметры, которые незначительно влияют на модель. Для этого воспользуемся критерием Акаике.

```{r}
step = stepAIC(model1_stand,direction = 'both')

step$anova

step_model = lm(log(medv) ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat,data=stand_Boston)

summary(step_model)
```

Новая модель:

+ R-squared составляет 0.7864

+ F-statistic составляет 104.3

С помощью функции stepAIC мы последовательно рассчитали AIC для некоторых моделей и удалили предикторы, которые незначительно влияют на модель, что позволило несколько увеличить R-squared.

### Задание 2. Проведем диагностику полной модели.
1. Проверим на мультиколлениарность, которую мы заметили ранее.
```{r}
library(car)
vif(model1_stand)
```

Для дальнейшей работы нужно удалить предикторы с VIF > 5 (rad, tax).

2. Проверим остатки:
```{r}
plot(model1_stand$residuals)
abline(h=0,col='red')
```
Мы видим что остатки распределены ненормально. 

3. Найдем выбросы:
```{r}

outlierTest(model1_stand)
```

Мы видим 4 выброса, влияющие на нашу модель.

Наибольший вклад вносит переменная lstat - -0.207344.

### Задание 3. Постройте график предсказаний стоимости от переменной, которая обладает
наибольшим по модулю коэффициентом.


```{r}
mod_hi <- lm(medv ~ as.numeric(lstat), data = stand_Boston)

test$predicted.medv <- predict(mod_hi,test)
pl1 <-test %>% 
  ggplot(aes(medv,predicted.medv)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Значения medv') +
  ylab('Предсказанные значения medv') +
  theme_bw()

ggplotly(pl1)
```



### Дополнительное задание.
Продолжим преобразовывать модель на НЕстандартизованных данных.
```{r}
# сделаем полную модель на НЕстандартизованных данных
model1 = lm(log(medv)~., data= Boston)
summary(model1_stand)

par(mfrow=c(2,2))
plot(model1)
```

Новая модель:

+ R-squared составляет 0.7841

+ F-statistic составляет 142.1

```{r}
step = stepAIC(model1,direction = 'both')

step$anova

step_model = lm(log(medv) ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat,data=Boston)

summary(step_model)
```

Новая модель:

+ R-squared составляет 0.7844

+ F-statistic составляет 168.1

```{r}
library(car)
vif(step_model)
```

```{r}
outlierTest(step_model)
```

У нас есть 4 выброса, которые мы удалим.


```{r}
Boston_1 = Boston[-c(413,372, 373, 402),,drop=T]# drop the points
row.names(Boston_1)=1:nrow(Boston_1)

model2 = lm(log(medv)~crim+zn+chas+nox+dis+ptratio+black+lstat,data=Boston)
summary(model2)
plot(model2)
```


Новая модель:

+ R-squared составляет 0.7545
+ F-statistic составляет 195

Не смотря на то, что мы устранили мульитколлинеанрость и незначимые предикторы, предсказания модели стали немного хуже (75%).

```{r}
vif(model2)
```

Проведем диагностику модели
```{r}
# Assumption - mean of the residuals is = 0 
mean(model2$residuals)

```

```{r}
plot(model2$residuals)
abline(h=0,col='red')
```

Остатки распределены более меннее нормально.

Найдем выбросы.
```{r}
outlierTest(model2)
```

У нас есть два выброса, которые можно удалить.

```{r}
Boston_1 = Boston[-c(413,372),,drop=T]# drop the points
row.names(Boston_1)=1:nrow(Boston_1)

model3 = lm(log(medv) ~ crim + zn + chas + nox + rm + dis +
              ptratio + black + lstat,data=Boston_1)
par(mfrow=c(2,2))
plot(model3)
```

```{r}
summary(model3)
```

Новая модель:

* R-squared составляет 0.7899

* F-statistic составляет 211.1

После удаления выбросов, мы видим, что значение R-squared увеличилось. Однако один из параметров перестал быть значимым. 

```{r}
model3_step=stepAIC(model3,Boston_1,direction = 'both')
```

```{r}
model3_step$anov
```

```{r}
summary(model3_step)
```

Новая модель:

* R-squared составляет 0.7899

* F-statistic составляет 211.1

```{r}
par(mfrow=c(2,2))
plot(model3_step)
```

Мы видим, что нет разницы между model3 и model3_step, а также Р-квадрат имеет одинаковые значения. Теперь можем проанализировать выбросы модели.

```{r}
outlierTest(model3)
```

Мы видим 2 выбросы, которые мы можем выбросить и протестировать модель еще раз. 
```{r}
Boston_2=Boston_1[-c(371,400,373),,drop=T]
row.names(Boston_2)=1:nrow(Boston_2)
model4 = lm(log(medv) ~ crim + zn + chas + nox + rm + dis +
                 tax + ptratio + black + lstat,data=Boston_2)
summary(model4)
```

Новая модель:

* R-squared составляет 0.7929

* F-statistic составляет 192.4

```{r}
par(mfrow=c(2,2))
plot(model4)
```

Мы видим, что мы снова увеличили мощность нашей модели, но потеряли значение по другой переменной. Мы можем запустить stepAIC и посмотреть, сможем ли мы его удалить.

```{r}
outlierTest(model4)
```

Мы видим 4 выбросы, которые мы можем выбросить и протестировать модель еще раз.
```{r}
Boston_3 =Boston_2[-c(369,371,372,398),,drop=T]
row.names(Boston_3)=1:nrow(Boston_3)
Boston_3$chas <- as.factor(Boston_3$chas)
model5 = lm(log(medv) ~ crim + zn + chas + nox + rm + dis +
                 tax + ptratio + black + lstat,data=Boston_3)
summary(model5)
```

Новая модель:

* R-squared составляет 0.8163

* F-statistic составляет 221.4
```{r}
par(mfrow=c(2,2))
plot(model5)
```

Мы улучшили еще немного нашу модель и теперь она объясняет 82% изменчивости. Нет предела совершенству, меня устраивает модель, объясняющая 82% изменчивости. 
Наибольший вклад вносят параметры: количество комнат, количество оксида азота, отношение учеников к учителям. Так что я бы посоветовала клиенту, строить большие дома рядом со школой и с парком неподалеку. 

### Финальный анализ модели.
```{r}
residuals <- data.frame('Residuals' = model5$residuals)
res_hist <- ggplot(residuals, aes(x=Residuals)) + geom_histogram(color='black', fill='skyblue') + ggtitle('Histogram of Residuals')
res_hist
```

Остатки распределены нормально.
```{r}
plot(model5, col='Sky Blue')
```

```{r}
# пересчитаем тестовый датасет
set.seed(123)
split <- sample.split(Boston,SplitRatio = 0.75) #assigns booleans to a new coloumn based on the split ratio
train <- subset(Boston,split==TRUE)
test <- subset(Boston,split==FALSE)

test$predicted.medv <- predict(model5,test)
pl1 <-test %>% 
  ggplot(aes(medv,predicted.medv)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Значения medv') +
  ylab('Предсказанные значения medv') +
  theme_bw()

ggplotly(pl1)
```

### 